---
id: 3
sidebar_position: 3
title: 第三节 人类反馈强化学习：对齐人类价值观
hide_title: true
---

## 微调后的障碍与对齐的必要性
即便经过微调，大模型在现实使用中仍会遇到挑战。它可能输出与人类价值观不符的信息，如“三观不正”的回答，给企业带来风险；也可能在一些难以精确定义的任务中表现不足，例如“是否幽默”或“是否生动”的判断。这些问题显示，单靠微调仍不足以保证模型输出符合人类的需求和价值观，**因此必须增加“对齐”环节，让模型在安全性、风格和价值导向上更贴近人类偏好**。

## 强化学习
对齐的核心方法是“**人类反馈强化学习**”（RLHF - Reinforcement Learning from Human Feedback），即结合强化学习与人类评价。强化学习的本质是“**奖励驱动**”，模型通过与环境交互获得奖励信号，逐渐形成最优策略，类似动物在驯养员的奖励下学会特定动作。它的优势在于**自适应学习**，能应对动态环境；不需要大量标注数据即可自我探索；还能规划复杂、长期的决策。因此，强化学习已在游戏、自动驾驶等场景中展现出超越人类的表现。

然而，强化学习也有明显局限。首先，现实任务中的**奖励信号往往模糊或难以定义**，例如如何量化“幽默”。其次，奖励函数若设计不当，可能让模型偏离目标，例如只奖励“安静学习”的学生会压制提问。最后，模型可能采取不道德或不安全的策略来追求奖励，如作弊以换取高分。因此，单纯依赖算法奖励模型并不可靠。

## 人类反馈
为克服这些问题，引入人类反馈成为关键。人类反馈可以表现为专家示范、直接评价、纠正性指导、明确指令，甚至是用户的隐性行为（如点击或停留时间）。这种介入带来了四大益处：
- 一是提供价值导向，使模型具备明确的伦理与安全边界；
- 二是通过人类干预加快学习进程，减少试错成本；
- 三是增强可解释性，提高信任度；
- 四是支持个性化服务，满足不同用户的需求。在医疗、法律等高风险领域，人类反馈尤为重要，因为它能确保大模型的决策符合专业标准和社会价值。

## 人类反馈强化学习的三个步骤
人类反馈强化学习（RLHF）主要分为三个核心环节。
- 第一步是**反馈收集**，模型生成回答后，人类需要对结果进行评价。这些反馈可以通过打分、问卷或在线互动获得。为了保证反馈有效，参与人群必须多样，任务类型也要广泛，并且要用科学的抽样方法来避免偏差。
- 第二步是**奖励模型训练**，研究人员利用收集到的反馈数据训练一个“奖励模型”，它能像裁判一样判断答案的好坏，在后续训练中相当于模型的“驯养员”。
- 第三步是**策略优化**，模型在奖励模型的指导下调整生成方式，常用的工具是PPO算法。PPO的优点是更新稳健，不会一下子大幅度偏移，而是像厨师不断微调食谱，多次试错，最终找到最佳口味。

尽管这种方法能显著提升模型质量，但在人力与算力资源上代价极高，尤其是反馈收集和奖励模型训练两个环节，往往是整个流程中最昂贵的部分。

## 直接偏好优化：省去奖励模型的新思路
DPO（Direct Preference Optimization，直接偏好优化）是一种新兴的训练方法，它和传统的RLHF相比最大的不同，就是不再需要单独训练奖励模型。**在RLHF的PPO（Proximal Policy Optimization）流程里，需要先收集人类反馈，再训练一个奖励模型，然后再用这个奖励模型指导模型优化；而DPO直接跳过奖励模型这一步，收集到反馈后就能直接用来优化模型**。这样一来，训练流程更简化，所需的资源和时间都更少，整体效率更高。

缺点在于，**DPO对反馈数据的质量要求更严格**，尤其是在复杂任务中，要想让模型准确捕捉人类偏好，就必须投入大量高质量的数据收集工作，成本依然不小。值得注意的是，除了DPO，研究者们还在探索TDPO（在词元级别进行优化）和ORPO（通过代理奖励进行优化）等方法。总体趋势是，模型对齐技术正在不断演进，目标是让大模型更快、更高效地学习人类价值观，同时减少昂贵的资源消耗。

## 对齐后的人类反馈强化学习成效
在人类反馈强化学习（RLHF）引入后，大模型表现出了四个显著提升：
1. 首先是回应更**翔实**，大模型能够生成更为详细、扩展的信息，这是RLHF让模型学会更贴近人类期望的直接效果
2. 其次是回应更**公正**，模型在回答时倾向于保持平衡，而不是偏向某一方
3. 第三是能够**拒绝不当问题**，无论是涉及违法违规，还是有人刻意引导模型给出危险回答，RLHF都让模型学会“踩刹车”，直接拒绝
4. 最后是能**分辨知识范围**，即隐式区分哪些问题属于自己掌握的领域，哪些超出范围，从而避免给出虚假或不负责任的回答。这些改进让大模型在进入生产和实际应用时更可靠。

这种改进的核心在于“人类反馈+强化学习”的结合。通过奖励模型生成信号，模型得以不断更新策略，而PPO算法则保证了学习过程的稳定性与渐进性。而随着DPO等新方法的探索，研究者希望进一步简化流程、降低成本。整体来看，人类反馈强化学习不仅提升了模型的回答质量和价值对齐能力，也让大模型在安全性和可控性上迈出了关键一步。